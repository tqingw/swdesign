# Application properties for the Spring Boot application
logging.level.root=WARN
logging.level.org.springframework.web=INFO
logging.level.csd.swdesign=DEBUG
# The client (browser, REST caller, etc.) receives the error status, message, and timestamp, but no internal Java stacktrace.
server.error.include-stacktrace=never

# The base URL for the Ollama API
spring.ai.ollama.base-url=http://localhost:11434

# Configure the chat client to use a specific model and temperature
spring.ai.ollama.chat.options.model=phi4-mini
# Set the temperature for the chat model
# A lower temperature makes the model's responses more deterministic
# while a higher temperature allows for more creative responses.
# Adjust this value based on your application's needs.
spring.ai.ollama.chat.options.temperature=0.7

# Uncomment the following line to use OpenAI instead of Ollama
# You will need to set the OPENAI_KEY environment variable
# to your OpenAI API key.
# spring.ai.openai.api-key=${OPENAI_KEY}
